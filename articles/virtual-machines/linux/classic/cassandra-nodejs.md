---
title: Uruchom Cassandra z systemem Linux na platformie Azure | Dokumentacja firmy Microsoft
description: "Jak uruchomić Cassandra klastra w systemie Linux w maszynach wirtualnych platformy Azure z poziomu aplikacji Node.js"
services: virtual-machines-linux
documentationcenter: nodejs
author: tomarcher
manager: routlaw
editor: 
tags: azure-service-management
ms.assetid: 30de1f29-e97d-492f-ae34-41ec83488de0
ms.service: virtual-machines-linux
ms.workload: infrastructure-services
ms.tgt_pltfrm: vm-linux
ms.devlang: na
ms.topic: article
ms.date: 08/17/2017
ms.author: tarcher
ms.openlocfilehash: 1ff3d77ced6c9d90029b251490c05e52d9b43515
ms.sourcegitcommit: 18ad9bc049589c8e44ed277f8f43dcaa483f3339
ms.translationtype: MT
ms.contentlocale: pl-PL
ms.lasthandoff: 08/29/2017
---
# <a name="running-cassandra-with-linux-on-azure-and-accessing-it-from-nodejs"></a>Uruchamianie rozwiązania Cassandra w systemie Linux na platformie Azure i uzyskiwanie do niej dostępu na platformie Node.js
> [!IMPORTANT] 
> Platforma Azure ma dwa różne modele wdrażania do tworzenia i pracy z zasobami: [Resource Manager i Model Klasyczny](../../../resource-manager-deployment-model.md). W tym artykule omówiono przy użyciu klasycznego modelu wdrożenia. Firma Microsoft zaleca, aby w przypadku większości nowych wdrożeń korzystać z modelu opartego na programie Resource Manager. Zobacz szablony Menedżera zasobów dla [Datastax Enterprise](https://azure.microsoft.com/documentation/templates/datastax) i [Spark klastra i Cassandra na CentOS](https://azure.microsoft.com/documentation/templates/spark-and-cassandra-on-centos/).

## <a name="overview"></a>Omówienie
Microsoft Azure to platforma chmury Otwórz uruchamianego zarówno do firmy Microsoft jako oprogramowania również w innych niż Microsoft, w tym systemów operacyjnych, serwerów aplikacji, oprogramowanie pośredniczące obsługi wiadomości, a także baz danych SQL i NoSQL z obu modeli handlowych i otwórz źródła. Tworzenie usług odporne na chmur publicznych, w tym na platformie Azure wymaga staranne planowanie i architektura zamierzonego dla obu serwerów aplikacji jako warstwy dobrze magazynu. Naturalnie Cassandra w magazynie rozproszonym architektura ułatwia tworzenie wysokiej dostępności systemów, które są odporne na uszkodzenia w przypadku awarii klastra. Cassandra jest skali chmury bazy danych NoSQL utrzymywana przez Foundation oprogramowania Apache cassandra.apache.org; Cassandra jest napisany w języku Java i dlatego działa zarówno na system Windows, jak i Linux platformy.

Ten artykuł koncentruje się pokazanie Cassandra wdrożenia na Ubuntu jako klaster pojedynczych i wielu danych Centrum wykorzystaniu maszyny wirtualne Microsoft Azure i sieci wirtualnych. Wdrażanie klastra, w przypadku obciążeń produkcyjnych zoptymalizowanych pod kątem jest poza zakres tego artykułu, ponieważ wymaga ona konfiguracji węzła wielu dysków, projekt topologia pierścienia odpowiednie i danych modelowania do obsługi replikacji potrzebne, spójność danych, przepływności i wysoki wymagania dotyczące dostępności.

Ten artykuł ma podstawowych podejście do wyświetlenia, co jest zaangażowane w tworzenie klastra Cassandra porównywany Docker, Chef lub Puppet, które mogą ułatwić wdrożenie infrastruktury partii.  

## <a name="the-deployment-models"></a>Modele wdrażania
Sieci Microsoft Azure umożliwia wdrożenie izolowanego prywatnej klastrów, dostępu do których może być ograniczone do osiągnięcia zabezpieczenia sieci szczegółowe.  Ponieważ ten artykuł dotyczy przedstawiający wdrażanie Cassandra na poziomie podstawowym, firma Microsoft nie koncentruje się na poziomu spójności i projektu pamięci masowej optymalnej przepływności. Poniżej przedstawiono listę wymagań sieciowych dotyczących naszych hipotetyczny klastra:

* Systemy zewnętrzne nie może uzyskać dostępu Cassandra bazy danych z lub poza Azure
* Cassandra klastra musi być za modułem równoważenia obciążenia dla ruchu thrift
* Wdrażanie węzłów Cassandra na dwie grupy w każdej centrum danych dostępności klastra rozszerzone
* Zablokowanie klastra tak że tylko do farmy serwerów aplikacji ma dostęp do bazy danych bezpośrednio
* Brak publicznego punktów końcowych sieci innych niż SSH
* Każdy węzeł Cassandra musi stałym wewnętrznego adresu IP

Cassandra można wdrożyć jeden region platformy Azure lub wielu regionach, oparte na Rozproszony charakter obciążenia. Model wdrażania w przypadku nadającego się do obsługi użytkowników końcowych bliżej do określonej lokalizacji geograficznej, za pomocą tej samej infrastrukturze Cassandra. Cassandra dla wbudowanego węzła replikacji ma opieki synchronizacji wielu wzorca zapisuje pochodzących z wielu centrów danych i przedstawia widok spójności danych aplikacji. W przypadku wdrożenia może również pomóc z umożliwiają ograniczenie ryzyka związanego z szerszych awarie usługi Azure. Topologii replikacji i spójności dostosowywalne w Cassandra pomoże spełnia zróżnicowane potrzeby RPO aplikacji.

### <a name="single-region-deployment"></a>Wdrażanie jednego regionu
Firma Microsoft uruchomi z wdrożeniem pojedynczego regionu i podłączono learnings modelu w przypadku tworzenia. Sieci wirtualne platformy Azure będzie używany do utworzenia izolowanego podsieci spełnienia wymagań bezpieczeństwa sieci wymienione powyżej.  Używa procesu opisanego w tworzeniu wdrożenia jeden region, Ubuntu 14.04 LTS i Cassandra 2.08; Jednak proces można łatwo przyjąć innych wariantów systemu Linux. Poniżej przedstawiono niektóre właściwości systemowych wdrożenia w pojedynczym regionie.  

**Wysoka dostępność:** węzłów Cassandra pokazany na rysunku 1 są wdrażane dwa zestawy dostępności, aby węzły są rozkładane między wiele domen błędów w celu zapewnienia wysokiej dostępności. Maszyny wirtualne adnotowany przy każdym zestawie dostępności jest mapowana na 2 domen błędów.  Microsoft Azure wykorzystuje pojęcie domeny błędów do zarządzania a koncepcji uaktualniania domeny (np. hosta lub gościa systemu operacyjnego stosowanie poprawek/uaktualnień, uaktualnień aplikacji) nieplanowanego wyłączenia jednocześnie (np. awarii sprzętu lub oprogramowania) służy do zarządzania zaplanowany czas przestoju. Zobacz [odzyskiwania po awarii i wysoką dostępność aplikacji Azure](http://msdn.microsoft.com/library/dn251004.aspx) roli domenach awarii i uaktualniania w celu osiągnięcia wysokiej dostępności.

![Wdrażanie jednego regionu](./media/cassandra-nodejs/cassandra-linux1.png)

Rysunek 1: Wdrożenie pojedynczy region

Zauważ, że w momencie pisania tego Azure nie zezwalają jawne mapowanie grupy maszyn wirtualnych do domeny określonych błędów; w związku z tym nawet w przypadku modelu wdrażania pokazany na rysunku 1, jest statystycznie prawdopodobne, że wszystkie maszyny wirtualne mogą być mapowane na dwóch domen błędów zamiast cztery.

**Ruch Thrift równoważenia obciążenia:** bibliotek klienckich Thrift wewnątrz serwera sieci web Połącz się z klastrem za pośrednictwem wewnętrznego modułu równoważenia obciążenia. Wymaga to proces dodawania wewnętrznego modułu równoważenia obciążenia z podsiecią "data" (zobacz rysunek 1) w kontekście klastra Cassandra hosting usługi w chmurze. Po zdefiniowaniu jest wewnętrzny moduł równoważenia obciążenia, każdy węzeł wymaga punktu końcowego równoważenia obciążenia ma zostać dodana z adnotacji o nazwie usługi równoważenia obciążenia wcześniej zdefiniowanego zestawu o zrównoważonym obciążeniu. Zobacz [wewnętrzny równoważenia obciążenia Azure ](../../../load-balancer/load-balancer-internal-overview.md)więcej szczegółów.

**Ziarna klastra:** ważne jest, aby wybrać większości węzłów wysokiej dostępności DS, jak nowe węzły będą komunikować się z węzłów inicjatora umożliwia odnalezienie topologii klastra. Jeden węzeł z każdym zestawie dostępności jest wyznaczony jako węzłów inicjatora, aby uniknąć pojedynczego punktu awarii.

**Współczynnik replikacji i poziomu spójności:** Cassandra w kompilacji w wysokiej dostępności i danych trwałości charakteryzuje się współczynnik replikacji (RF - liczby kopii każdego wiersza przechowywane w klastrze) i poziomu spójności (liczba replik być odczytana/zapisana przed zwróceniem wyniku do obiektu wywołującego). Współczynnik replikacji jest określone podczas tworzenia przestrzeni KLUCZY (podobnie jak relacyjna baza danych), określić poziom spójności podczas CRUD zapytania. W dokumentacji Cassandra [Konfigurowanie spójności](http://www.datastax.com/documentation/cassandra/2.0/cassandra/dml/dml_config_consistency_c.html) szczegóły spójności i formułę obliczenia kworum.

Cassandra obsługuje dwa typy modeli integralności danych — spójność i spójność ostateczna; Współczynnik replikacji i poziomu spójności ze sobą określi, czy dane będą zgodne natychmiast po zakończeniu operacji zapisu lub będą zgodne po pewnym czasie. Na przykład określenie KWORUM, zgodnie z poziomu spójności będzie zawsze gwarantuje spójności danych dowolnego poziomu spójności, poniżej liczba replik do zapisania w celu osiągnięcia KWORUM (np. jeden) powoduje ostatecznie spójności danych.

Powyższy współczynnik replikacji 3 i KWORUM klastra 8 węzłów (2 węzły są odczytywane lub zapisywane spójności) poziomu spójności odczytu/zapisu, przełączniki teoretycznego utraty najwyżej 1 węzła dla grupy replikacji przed powiadamiania uruchamiania aplikacji Wystąpił błąd. Przy założeniu, że wszystkie spacje klucza również mieć równoważony odczytu/zapisu żądania.  Parametry, które będą używane dla klastra wdrożone są następujące:

Konfiguracja klastra Cassandra jednego regionu:

| Parametr klastra | Wartość | Uwagi |
| --- | --- | --- |
| Liczba węzłów (N) |8 |Całkowita liczba węzłów w klastrze |
| Współczynnik replikacji (RF) |3 |Liczba replik danego wiersza |
| Poziom zgodności (Zapisz) |QUORUM[(RF/2) +1) = 2 wynik formuły zostać zaokrąglona w dół |Zapisuje najwyżej 2 replik przed wysłaniem odpowiedzi do wywołującego 3 repliki są zapisywane w sposób spójny po pewnym czasie. |
| Poziom zgodności (odczyt) |KWORUM [(RF/2) + 1 = 2] jest zaokrąglana wynik formuły |Odczytuje 2 replik przed wysłaniem odpowiedzi do obiektu wywołującego. |
| Strategii replikacji |Zobacz NetworkTopologyStrategy [replikacji danych](http://www.datastax.com/documentation/cassandra/2.0/cassandra/architecture/architectureDataDistributeReplication_c.html) w Cassandra dokumentacji, aby uzyskać więcej informacji |Rozumie topologii wdrożenia i umieszcza replik w węzłach, tak, aby wszystkie repliki nie trafiają do tej samej stojak |
| Snitch |Zobacz GossipingPropertyFileSnitch [Snitches](http://www.datastax.com/documentation/cassandra/2.0/cassandra/architecture/architectureSnitchesAbout_c.html) w Cassandra dokumentacji, aby uzyskać więcej informacji |NetworkTopologyStrategy używa pojęcie snitch zrozumienie topologii. GossipingPropertyFileSnitch zapewnia lepszą kontrolę w mapowaniu każdego węzła do centrum danych i stojaku. Klaster używa następnie plotki propagację te informacje. To jest znacznie prostsza w dynamicznej ustawienie adresu IP względem PropertyFileSnitch |

**Azure zagadnienia dotyczące klastra Cassandra:** możliwości maszyny wirtualne Microsoft Azure korzysta z magazynu obiektów Blob platformy Azure dysku trwałości; Usługa Azure Storage zapisuje 3 replik każdego dysku, aby uzyskać wysoką trwałością. Oznacza to, każdy wiersz danych wstawione do tabeli Cassandra jest już zapisana w 3 repliki i dlatego spójność danych jest już wykonane obsługę nawet, jeśli współczynnik replikacji (RF) wynosi 1. Główny problem z współczynnik replikacji jest 1 jest aplikacji wystąpi przestój nawet w przypadku awarii jednego węzła Cassandra. Jednak jeśli węzeł nie działa dla problemów (np. sprzętu, błędów oprogramowania systemu) rozpoznawany przez kontroler sieci szkieletowej Azure, będzie dostarczać nowego węzła w tym miejscu przy użyciu tego samego dysków. Inicjowanie obsługi nowego węzła do zastąpi stare hasło może potrwać kilka minut.  Podobnie czynności zaplanowanej konserwacji, jak zmiany systemu operacyjnego gościa, uaktualnia Cassandra, i zmian w aplikacji Kontroler sieci szkieletowej Azure wykonuje wprowadzanie uaktualnień węzłów w klastrze.  Również uaktualnień stopniowych może potrwać kilka węzłów w dół w czasie, a tym samym klastrze może wystąpić krótki przestój w przypadku kilku partycji. Jednak dane nie zostaną utracone z powodu wbudowane funkcje nadmiarowości usługi Azure Storage.  

Dla systemów wdrożeniu na platformie Azure, która nie wymaga wysokiej dostępności (np. około 99,9 który jest odpowiednikiem 8.76 godzin na rok, zobacz [wysokiej dostępności](http://en.wikipedia.org/wiki/High_availability) szczegółowe informacje) można uruchamiać z RF = 1 i poziom spójności równej ONE.  W przypadku aplikacji o wysokiej dostępności wymagania, RF = 3 i poziomu spójności = KWORUM będą tolerować czas przestoju jednego z węzłów z replik. RF = 1 w przypadku tradycyjnych wdrożeń (np. lokalnego) nie można używać z powodu możliwej utracie danych wynikające z problemów, takich jak awarii dysku.   

## <a name="multi-region-deployment"></a>W przypadku wdrożenia
W Cassandra data center rozpoznają replikacji i spójność model opisane powyżej pomaga przy użyciu wdrażania w przypadku fabrycznej bez konieczności stosowania żadnych narzędzi zewnętrznych. Jest to całkowicie różnią się od tradycyjnych relacyjnych baz danych, gdzie instalację na potrzeby dublowania bazy danych dla wielu wzorców operacji zapisu może być dość złożone. Cassandra w regionie wielu — konfiguracja może ułatwić scenariusze użycia, takie jak następujące:

**Wdrożenia na podstawie bliskości:** aplikacje wielodostępne, wyczyść mapowania użytkowników dzierżawy-do-region, można korzystali przez klaster w przypadku małych opóźnień. Na przykład zarządzanie learning systemów dla instytucji edukacyjnych można wdrożyć klaster rozproszonych w regionach wschodnie stany USA i zachodnie stany USA do obsługi odpowiednich szkoły wyższe dla transakcyjnego, a także analizy. Dane mogą być lokalnie spójne w czasie odczyty i zapisy i może być ostatecznie spójne w obu regionach. Istnieją inne przykłady takich jak dystrybucji nośnika, handlu elektronicznego i niczego i wszystko, co służy geograficznie skoncentrowanego użytkownika podstawowego jest przypadek użycia dobrej dla tego modelu wdrażania.

**Wysoka dostępność:** nadmiarowość jest kluczowym czynnikiem w celu osiągnięcia wysokiej dostępności sprzętu i oprogramowania; więcej szczegółów, zobacz Tworzenie niezawodnej systemy chmur w systemie Microsoft Azure. W systemie Microsoft Azure tylko niezawodny sposób realizacji nadmiarowość wartość true, jest wdrażanie klastra w przypadku. Aplikacje można wdrażać w trybie aktywny aktywny lub aktywny / pasywny, a jeśli jeden z regionów działa, usługi Azure Traffic Manager można przekierowywania ruchu sieciowego na aktywnym regionem.  Przy użyciu wdrażania jednego regionu, w przypadku dostępności 99,9, region dwa wdrożenia może osiągnąć dostępności 99.9999 obliczone przez formułę: (1-(1-0.999) * (1-0.999)) * 100); zobacz dokument powyżej, aby uzyskać szczegółowe informacje.

**Odzyskiwanie po awarii:** Cassandra w przypadku klastra, jeśli przeznaczony prawidłowo, może wytrzymać przestojami centrum danych w wyniku katastrofy. Jeśli działa jeden region, można uruchomić aplikacji wdrożone do innych regionów obsługująca użytkowników końcowych. Podobnie jak wszelkie innych firm ciągłości implementacji aplikacja musi być odporny na błędy na utratę danych, na podstawie danych w potoku asynchroniczne. Jednak Cassandra sprawia, że odzyskiwania znacznie usprawnić niż czas trwania procesów odzyskiwania tradycyjne bazy danych. Na rysunku 2 przedstawiono typowe wdrożenie w przypadku modelu o ośmiu węzłów w każdym regionie. Zarówno regiony są dublowane obrazy wzajemnie dla tego samego symetrycznego; projekty rzeczywistych zależy od tego, typu obciążenia (np. transakcyjnej lub analitycznych), RPO, RTO, spójności danych oraz wymagań dotyczących dostępności.

![Obsługa wielu region wdrożenia](./media/cassandra-nodejs/cassandra-linux2.png)

Rysunek 2: W przypadku Cassandra wdrożenia

### <a name="network-integration"></a>Integracja sieci
Ustawia maszyn wirtualnych wdrożonych na sieciach prywatnych, znajduje się na dwóch regionach komunikuje się ze sobą przy użyciu tunelu VPN. Tunel VPN łączy dwie bramy oprogramowanie udostępniane w trakcie procesu wdrażania sieci. Zarówno regiony mają podobną architekturę sieci pod względem podsieci "web" i "dane"; Sieć platformy Azure umożliwia tworzenie tyle podsieci zgodnie z potrzebami i stosować listy ACL, odpowiednio do potrzeb zabezpieczeń sieci. Podczas projektowania topologii klastra między opóźnienie komunikacji centrum danych i ekonomiczne wpływu na ruch sieciowy należy wziąć pod uwagę.

### <a name="data-consistency-for-multi-data-center-deployment"></a>Spójność danych wdrożenia Centrum usługi danych
Rozproszone wdrożeń muszą być świadome wpływu Topologia klastra na przepływność i wysokiej dostępności. Należy wybrać w taki sposób, że kworum nie zależy od dostępności wszystkich centrów danych, należy RF i poziomu spójności.
Wymagające wysokiej spójności systemu LOCAL_QUORUM spójności poziomu (odczyty i zapisy) będzie upewnij się, że lokalne odczyty i zapisy są spełnione z lokalnych węzłów podczas, gdy dane są replikowane asynchronicznie w centrach danych zdalnych.  Tabela 2 zawiera podsumowanie szczegółów konfiguracji klastra w przypadku opisane w dalszej części zapisu w górę.

**Konfiguracja klastra Cassandra dwa regionu**

| Parametr klastra | Wartość | Uwagi |
| --- | --- | --- |
| Liczba węzłów (N) |8 + 8 |Całkowita liczba węzłów w klastrze |
| Współczynnik replikacji (RF) |3 |Liczba replik danego wiersza |
| Poziom zgodności (Zapisz) |LOCAL_QUORUM [(sum(RF)/2) +1) = 4] wynik formuły zostać zaokrąglona w dół |węzły 2 pierwszego centrum danych będą zapisywane synchronicznie; dodatkowe węzły 2 potrzebne do kworum będą zapisywane asynchronicznie 2 centrum danych. |
| Poziom zgodności (odczyt) |LOCAL_QUORUM ((RF/2) + 1) = 2, wynik formuły zostać zaokrąglona w dół |Żądań odczytu są spełnione przy tylko jeden region; 2 węzły są odczytywane przed wysłaniem odpowiedzi do klienta. |
| Strategii replikacji |Zobacz NetworkTopologyStrategy [replikacji danych](http://www.datastax.com/documentation/cassandra/2.0/cassandra/architecture/architectureDataDistributeReplication_c.html) w Cassandra dokumentacji, aby uzyskać więcej informacji |Rozumie topologii wdrożenia i umieszcza replik w węzłach, tak, aby wszystkie repliki nie trafiają do tej samej stojak |
| Snitch |Zobacz GossipingPropertyFileSnitch [Snitches](http://www.datastax.com/documentation/cassandra/2.0/cassandra/architecture/architectureSnitchesAbout_c.html) w Cassandra dokumentacji, aby uzyskać więcej informacji |NetworkTopologyStrategy używa pojęcie snitch zrozumienie topologii. GossipingPropertyFileSnitch zapewnia lepszą kontrolę w mapowaniu każdego węzła do centrum danych i stojaku. Klaster używa następnie plotki propagację te informacje. To jest znacznie prostsza w dynamicznej ustawienie adresu IP względem PropertyFileSnitch |

## <a name="the-software-configuration"></a>KONFIGURACJA OPROGRAMOWANIA
Następujące wersje oprogramowania są używane podczas wdrażania:

<table>
<tr><th>Oprogramowanie</th><th>Element źródłowy</th><th>Wersja</th></tr>
<tr><td>ŚRODOWISKA JRE    </td><td>[ŚRODOWISKA JRE 8](http://www.oracle.com/technetwork/java/javase/downloads/server-jre8-downloads-2133154.html) </td><td>8U5</td></tr>
<tr><td>JNA    </td><td>[JNA](https://github.com/twall/jna) </td><td> 3.2.7</td></tr>
<tr><td>Cassandra</td><td>[Apache Cassandra 2.0.8](http://www.apache.org/dist/cassandra/2.0.8/apache-cassandra-2.0.8-bin.tar.gz)</td><td> 2.0.8</td></tr>
<tr><td>Ubuntu    </td><td>[Microsoft Azure](https://azure.microsoft.com/) </td><td>14.04 LTS</td></tr>
</table>

Ponieważ pobieranie środowiska JRE wymaga ręcznego akceptacji licencji Oracle, można uprościć wdrażanie, Pobierz wymagane oprogramowanie do pulpitu na przekazywanie dalej do obrazu szablonu Ubuntu, firma Microsoft będzie tworzony jako macierzystych do wdrożenia klastra.

Pobierz powyższego oprogramowania w katalogu pobierania dobrze znanego (np. %TEMP%/downloads w systemie Windows lub ~/Downloads na większości dystrybucje systemu Linux lub Mac.) na komputerze lokalnym.

### <a name="create-ubuntu-vm"></a>TWORZENIE MASZYNY WIRTUALNEJ SYSTEMU UBUNTU
W tym kroku procesu zostanie utworzony obraz Ubuntu z wstępnie wymagane oprogramowanie, aby obraz mogą być ponownie używane do inicjowania obsługi kilka węzłów Cassandra.  

#### <a name="step-1-generate-ssh-key-pair"></a>Krok 1: Generuje parę kluczy SSH
Platforma Azure wymaga X509 klucza publicznego, który jest PEM lub DER zakodowane w czasie inicjowania obsługi administracyjnej. Generowanie pary kluczy publiczny/prywatny, korzystając z instrukcji znajdujący się w sposób używanie SSH z systemem Linux na platformie Azure. Jeśli planujesz użyć putty.exe jako klienta SSH w systemie Windows lub Linux, należy przekonwertować PEM zakodowane RSA klucz prywatny do formatu PPK, używając puttygen.exe; odpowiednie instrukcje znajdują się w powyższym strony sieci web.

#### <a name="step-2-create-ubuntu-template-vm"></a>Krok 2: Tworzenie Ubuntu szablonu maszyny Wirtualnej
Aby utworzyć szablon maszyny Wirtualnej, zaloguj się do klasycznego portalu Azure i użyj następującej procedury: kliknij przycisk Nowy, obliczeń, maszyny WIRTUALNEJ, z GALERII, UBUNTU, Ubuntu Server 14.04 LTS, a następnie kliknij strzałkę w prawo. Samouczek, który opisuje metodę utworzyć Maszynę wirtualną systemu Linux, zobacz Tworzenie maszyny wirtualnej systemem Linux.

Na ekranie "konfiguracja maszyny wirtualnej" #1, wprowadź następujące informacje:

<table>
<tr><th>NAZWA POLA              </td><td>       WARTOŚĆ POLA               </td><td>         UWAGI                </td><tr>
<tr><td>DATA WYDANIA WERSJI    </td><td> Wybierz datę z listy rozwijanej</td><td></td><tr>
<tr><td>NAZWA MASZYNY WIRTUALNEJ    </td><td> Szablon przypadku                   </td><td> Jest to nazwa hosta maszyny wirtualnej </td><tr>
<tr><td>WARSTWY                     </td><td> STANDARDOWA                           </td><td> Pozostaw wartość domyślną              </td><tr>
<tr><td>ROZMIAR                     </td><td> A1                              </td><td>Wybierz maszynę Wirtualną, w zależności od potrzeb operacji We/Wy; w tym celu pozostaw wartość domyślną </td><tr>
<tr><td> NOWĄ NAZWĘ UŻYTKOWNIKA             </td><td> localadmin                       </td><td> "Administrator" jest zastrzeżoną nazwą użytkownika w xx 12 Ubuntu i po</td><tr>
<tr><td> UWIERZYTELNIANIE         </td><td> Kliknij pole wyboru                 </td><td>Sprawdź, czy chcesz zabezpieczyć za pomocą klucza SSH </td><tr>
<tr><td> CERTYFIKAT             </td><td> Nazwa pliku certyfikatu klucza publicznego </td><td> Użyj klucza publicznego, wcześniej wygenerowany</td><tr>
<tr><td> Nowe hasło    </td><td> Silne hasło </td><td> </td><tr>
<tr><td> Potwierdź hasło    </td><td> Silne hasło </td><td></td><tr>
</table>

Na ekranie "konfiguracja maszyny wirtualnej" #2, wprowadź następujące informacje:

<table>
<tr><th>NAZWA POLA             </th><th> WARTOŚĆ POLA                       </th><th> UWAGI                                 </th></tr>
<tr><td> USŁUGI W CHMURZE    </td><td> Utwórz nową usługę w chmurze    </td><td>Usługa w chmurze jest zasoby obliczeniowe kontenera, takich jak maszyny wirtualne</td></tr>
<tr><td> NAZWA DNS USŁUGI W CHMURZE    </td><td>ubuntu template.cloudapp.net    </td><td>Nadaj nazwę modułu równoważenia obciążenia o niesprecyzowanym maszyny</td></tr>
<tr><td> REGION/GRUPY KOLIGACJI/SIECI WIRTUALNEJ </td><td>    Zachodnie stany USA    </td><td> Wybierz region, z której uzyskują dostęp do klastra Cassandra aplikacji sieci web</td></tr>
<tr><td>KONTO MAGAZYNU </td><td>    Użyj domyślnej    </td><td>Użyj domyślnego konta magazynu lub wstępnie utworzone konto magazynu w określonym regionie</td></tr>
<tr><td>ZESTAW DOSTĘPNOŚCI </td><td>    Brak </td><td>    Pozostaw to pole puste</td></tr>
<tr><td>PUNKTY KOŃCOWE    </td><td>Użyj domyślnej </td><td>    Użyj domyślnej konfiguracji SSH </td></tr>
</table>

Kliknij strzałkę w prawo, pozostaw wartości domyślne na ekranie #3 i kliknij przycisk "Sprawdź", aby ukończyć proces tworzenia maszyny Wirtualnej. Po kilku minutach maszyny Wirtualnej o nazwie "ubuntu-template" powinna być w stanie "uruchomiona".

### <a name="install-the-necessary-software"></a>ZAINSTALUJ OPROGRAMOWANIE NIEZBĘDNE
#### <a name="step-1-upload-tarballs"></a>Krok 1: Przekazywanie tarballs
Przy użyciu protokołu scp lub pscp, skopiuj pobrane wcześniej oprogramowanie do katalogu ~/downloads w następującym formacie polecenia:

##### <a name="pscp-server-jre-8u5-linux-x64targz-localadminhk-cas-templatecloudappnethomelocaladmindownloadsserver-jre-8u5-linux-x64targz"></a>pscp server-środowiska jre-8u5-linux-x64.tar.gzlocaladmin@hk-cas-template.cloudapp.net:/home/localadmin/downloads/server-jre-8u5-linux-x64.tar.gz
Powtórz powyższe polecenie dla środowiska JRE również podobnie jak w przypadku Cassandra usługi bits.

#### <a name="step-2-prepare-the-directory-structure-and-extract-the-archives"></a>Krok 2: Przygotowanie strukturę katalogów i wyodrębniać archiwum
Zaloguj się do maszyny Wirtualnej i Utwórz strukturę katalogów i Wyodrębnij oprogramowania jako superużytkownika przy użyciu poniższego skryptu bash:

    #!/bin/bash
    CASS_INSTALL_DIR="/opt/cassandra"
    JRE_INSTALL_DIR="/opt/java"
    CASS_DATA_DIR="/var/lib/cassandra"
    CASS_LOG_DIR="/var/log/cassandra"
    DOWNLOADS_DIR="~/downloads"
    JRE_TARBALL="server-jre-8u5-linux-x64.tar.gz"
    CASS_TARBALL="apache-cassandra-2.0.8-bin.tar.gz"
    SVC_USER="localadmin"

    RESET_ERROR=1
    MKDIR_ERROR=2

    reset_installation ()
    {
       rm -rf $CASS_INSTALL_DIR 2> /dev/null
       rm -rf $JRE_INSTALL_DIR 2> /dev/null
       rm -rf $CASS_DATA_DIR 2> /dev/null
       rm -rf $CASS_LOG_DIR 2> /dev/null
    }
    make_dir ()
    {
       if [ -z "$1" ]
       then
          echo "make_dir: invalid directory name"
          exit $MKDIR_ERROR
       fi

       if [ -d "$1" ]
       then
          echo "make_dir: directory already exists"
          exit $MKDIR_ERROR
       fi

       mkdir $1 2>/dev/null
       if [ $? != 0 ]
       then
          echo "directory creation failed"
          exit $MKDIR_ERROR
       fi
    }

    unzip()
    {
       if [ $# == 2 ]
       then
          tar xzf $1 -C $2
       else
          echo "archive error"
       fi

    }

    if [ -n "$1" ]
    then
       SVC_USER=$1
    fi

    reset_installation
    make_dir $CASS_INSTALL_DIR
    make_dir $JRE_INSTALL_DIR
    make_dir $CASS_DATA_DIR
    make_dir $CASS_LOG_DIR

    #unzip JRE and Cassandra
    unzip $HOME/downloads/$JRE_TARBALL $JRE_INSTALL_DIR
    unzip $HOME/downloads/$CASS_TARBALL $CASS_INSTALL_DIR

    #Change the ownership to the service credentials

    chown -R $SVC_USER:$GROUP $CASS_DATA_DIR
    chown -R $SVC_USER:$GROUP $CASS_LOG_DIR
    echo "edit /etc/profile to add JRE to the PATH"
    echo "installation is complete"


Po wklejeniu tego skryptu do okna vim upewnij się usunąć powrotu karetki ("\r") przy użyciu następującego polecenia:

    tr -d '\r' <infile.sh >outfile.sh

#### <a name="step-3-edit-etcprofile"></a>Krok 3: Edytowanie profilu itp.
Dołącz na końcu następujących:

    JAVA_HOME=/opt/java/jdk1.8.0_05
    CASS_HOME= /opt/cassandra/apache-cassandra-2.0.8
    PATH=$PATH:$HOME/bin:$JAVA_HOME/bin:$CASS_HOME/bin
    export JAVA_HOME
    export CASS_HOME
    export PATH

#### <a name="step-4-install-jna-for-production-systems"></a>Krok 4: Instalacja JNA dla systemów produkcyjnych.
Użyj poniższej sekwencji poleceń: następujące polecenie zainstaluje jna 3.2.7.jar i jna platformy 3.2.7.jar do katalogu /usr/share.java sudo stanie get zainstalować libjna java

Utwórz łącza symbolicznego w katalogu CASS_HOME/lib $ tak, aby Cassandra uruchamiania skryptu można znaleźć te słoików:

    ln -s /usr/share/java/jna-3.2.7.jar $CASS_HOME/lib/jna.jar

    ln -s /usr/share/java/jna-platform-3.2.7.jar $CASS_HOME/lib/jna-platform.jar

#### <a name="step-5-configure-cassandrayaml"></a>Krok 5: Konfigurowanie cassandra.yaml
Edytuj cassandra.yaml na każdej maszynie Wirtualnej w celu uwzględnienia konfiguracji wymagane przez wszystkich maszyn wirtualnych [firma Microsoft będzie dostosować to podczas udostępniania rzeczywistego]:

<table>
<tr><th>Nazwa pola   </th><th> Wartość  </th><th>    Uwagi </th></tr>
<tr><td>nazwa_klastra </td><td>    "CustomerService"    </td><td> Użyj nazwy, która odzwierciedla wdrożenia</td></tr>
<tr><td>listen_address    </td><td>[pozostaw to pole puste]    </td><td> Usuń "localhost" </td></tr>
<tr><td>rpc_addres   </td><td>[pozostaw to pole puste]    </td><td> Usuń "localhost" </td></tr>
<tr><td>ziarna    </td><td>"10.1.2.4, 10.1.2.6, 10.1.2.8"    </td><td>Lista wszystkich adresów IP, które są oznaczone jako dane.</td></tr>
<tr><td>endpoint_snitch </td><td> org.apache.cassandra.locator.GossipingPropertyFileSnitch </td><td> To jest używany przez NetworkTopologyStrateg dla wnioskowanie centrum danych i stojaku maszyny Wirtualnej</td></tr>
</table>

#### <a name="step-6-capture-the-vm-image"></a>Krok 6: Przechwytywanie obrazu maszyny Wirtualnej
Zaloguj się do maszyny wirtualnej przy użyciu nazwy hosta (hk-urzędów certyfikacji template.cloudapp.net) i klucz prywatny SSH wcześniej utworzony. Zobacz temat jak używanie SSH z systemem Linux na platformie Azure, aby uzyskać więcej informacji na temat sposobu Zaloguj się za pomocą polecenia ssh lub putty.exe.

Wykonania następującej akcji w celu przechwycenia obrazu:

##### <a name="1-deprovision"></a>1. Deprovision
Użyj polecenia "sudo agenta waagent — deprovision + użytkownika" Aby usunąć określone informacje o wystąpieniu maszyny wirtualnej. Zobacz dla [Przechwytywanie maszyny wirtualnej systemu Linux](capture-image.md) do użycia jako szablon więcej szczegółów na temat procesu przechwytywania obrazu.

##### <a name="2-shutdown-the-vm"></a>2: Zamknij maszynę Wirtualną
Upewnij się, że maszyna wirtualna zostanie wyróżniona, a następnie kliknij łącze zamknięcia z dolnym pasku poleceń.

##### <a name="3-capture-the-image"></a>3: Przechwytywanie obrazu
Upewnij się, że maszyna wirtualna zostanie wyróżniona, a następnie kliknij łącze PRZECHWYTYWANIA z dolnym pasku poleceń. Na następnym ekranie nadaj nazwę obrazu (np. hk-cas-2-08-ub-14-04-2014071), odpowiednich opis obrazu, a następnie kliknij znacznik "wyboru", aby zakończyć proces PRZECHWYTYWANIA.

To może zająć kilka sekund i obraz powinien być dostępny w sekcji Moje obrazy Galeria obrazów. Źródłowej maszyny Wirtualnej zostaną automatycznie usunięte pomyślnie przechwycony obraz. 

## <a name="single-region-deployment-process"></a>Proces wdrażania pojedynczy Region
**Krok 1: Utwórz sieć wirtualną** Zaloguj się do portalu Azure i utworzyć sieć wirtualną (klasyczne) z atrybutami pokazano w poniższej tabeli. Zobacz [Utwórz sieć wirtualną (klasyczne) przy użyciu portalu Azure](../../../virtual-network/virtual-networks-create-vnet-classic-pportal.md) szczegółowy opis kroków procesu.      

<table>
<tr><th>Nazwa atrybutu maszyny Wirtualnej</th><th>Wartość</th><th>Uwagi</th></tr>
<tr><td>Nazwa</td><td>sieć wirtualna — przypadku zachód nam</td><td></td></tr>
<tr><td>Region</td><td>Zachodnie stany USA</td><td></td></tr>
<tr><td>Serwery DNS</td><td>Brak</td><td>Zignoruj ten komunikat, ponieważ nie używamy serwera DNS</td></tr>
<tr><td>Przestrzeń adresów</td><td>10.1.0.0/16</td><td></td></tr>    
<tr><td>Uruchamianie adresu IP</td><td>10.1.0.0</td><td></td></tr>    
<tr><td>CIDR </td><td>/16 (65531)</td><td></td></tr>
</table>

Dodaj następujące podsieci:

<table>
<tr><th>Nazwa</th><th>Uruchamianie adresu IP</th><th>CIDR</th><th>Uwagi</th></tr>
<tr><td>sieci Web</td><td>10.1.1.0</td><td>/24 (251)</td><td>Podsieć dla kolektywu serwerów sieci web</td></tr>
<tr><td>Dane</td><td>10.1.2.0</td><td>/24 (251)</td><td>Podsieć dla węzłów bazy danych</td></tr>
</table>

Dane i podsieci w sieci Web mogą być chronione przy użyciu grup zabezpieczeń sieci zakres wykracza poza zakres tego artykułu.  

**Krok 2: Maszyny wirtualne należy** przy użyciu obrazu utworzonego wcześniej, firma Microsoft utworzy następujące maszyny wirtualne na serwerze chmury "hk-c-svc Zachód" i powiązać je do odpowiednich podsieci, jak pokazano poniżej:

<table>
<tr><th>Nazwa komputera    </th><th>Podsieć    </th><th>Adres IP    </th><th>Zestaw dostępności</th><th>DC/Rack</th><th>Inicjatora?</th></tr>
<tr><td>HK-c1-zachód us    </td><td>Dane    </td><td>10.1.2.4    </td><td>HK-c-aset-1    </td><td>DC = stojak WESTUS = szafa1 </td><td>Tak</td></tr>
<tr><td>HK-c2-zachód us    </td><td>Dane    </td><td>10.1.2.5    </td><td>HK-c-aset-1    </td><td>DC = stojak WESTUS = szafa1    </td><td>Nie </td></tr>
<tr><td>HK-c3-zachód us    </td><td>Dane    </td><td>10.1.2.6    </td><td>HK-c-aset-1    </td><td>DC = stojak WESTUS = szafa2    </td><td>Tak</td></tr>
<tr><td>HK-c4-zachód us    </td><td>Dane    </td><td>10.1.2.7    </td><td>HK-c-aset-1    </td><td>DC = stojak WESTUS = szafa2    </td><td>Nie </td></tr>
<tr><td>HK-c5-zachód us    </td><td>Dane    </td><td>10.1.2.8    </td><td>HK-c-aset-2    </td><td>DC = stojak WESTUS = szafa3    </td><td>Tak</td></tr>
<tr><td>HK-c6-zachód us    </td><td>Dane    </td><td>10.1.2.9    </td><td>HK-c-aset-2    </td><td>DC = stojak WESTUS = szafa3    </td><td>Nie </td></tr>
<tr><td>HK-c7-zachód us    </td><td>Dane    </td><td>10.1.2.10    </td><td>HK-c-aset-2    </td><td>DC = stojak WESTUS = rack4    </td><td>Tak</td></tr>
<tr><td>HK-c8-zachód us    </td><td>Dane    </td><td>10.1.2.11    </td><td>HK-c-aset-2    </td><td>DC = stojak WESTUS = rack4    </td><td>Nie </td></tr>
<tr><td>HK-w1 — zachód us    </td><td>sieci Web    </td><td>10.1.1.4    </td><td>HK p-aset-1    </td><td>                       </td><td>Nie dotyczy</td></tr>
<tr><td>HK-w2 — zachód us    </td><td>sieci Web    </td><td>10.1.1.5    </td><td>HK p-aset-1    </td><td>                       </td><td>Nie dotyczy</td></tr>
</table>

Utworzenie powyższej listy maszyn wirtualnych wymaga następujący proces:

1. Tworzenie usługi w chmurze pusta w określonym regionie
2. Utwórz maszynę Wirtualną z wcześniej przechwycony obraz i dołączenie go do sieci wirtualnej utworzone wcześniej; Powtórz tę czynność dla wszystkich maszyn wirtualnych
3. Dodawanie wewnętrznego modułu równoważenia obciążenia do usługi w chmurze i dołącz je do podsieci "dane"
4. Dla każdej maszyny Wirtualnej utworzone wcześniej Dodaj punkt końcowy równoważenia obciążenia dla ruchu thrift za pomocą zestawu o zrównoważonym obciążeniu podłączone do utworzonej wcześniej wewnętrznego modułu równoważenia obciążenia

Proces powyżej mogą być wykonywane przy użyciu klasycznego portalu Azure; Użyj maszynę z systemem Windows (Użyj maszyny Wirtualnej na platformie Azure, jeśli nie masz dostępu do komputera z systemem Windows), umożliwia automatycznie udostępnić wszystkich maszyn wirtualnych 8 następujący skrypt programu PowerShell.

**Lista 1: Skrypt programu PowerShell do obsługi maszyn wirtualnych**

        #Tested with Azure Powershell - November 2014
        #This powershell script deployes a number of VMs from an existing image inside an Azure region
        #Import your Azure subscription into the current Powershell session before proceeding
        #The process: 1. create Azure Storage account, 2. create virtual network, 3.create the VM template, 2. crate a list of VMs from the template

        #fundamental variables - change these to reflect your subscription
        $country="us"; $region="west"; $vnetName = "your_vnet_name";$storageAccount="your_storage_account"
        $numVMs=8;$prefix = "hk-cass";$ilbIP="your_ilb_ip"
        $subscriptionName = "Azure_subscription_name";
        $vmSize="ExtraSmall"; $imageName="your_linux_image_name"
        $ilbName="ThriftInternalLB"; $thriftEndPoint="ThriftEndPoint"

        #generated variables
        $serviceName = "$prefix-svc-$region-$country"; $azureRegion = "$region $country"

        $vmNames = @()
        for ($i=0; $i -lt $numVMs; $i++)
        {
           $vmNames+=("$prefix-vm"+($i+1) + "-$region-$country" );
        }

        #select an Azure subscription already imported into Powershell session
        Select-AzureSubscription -SubscriptionName $subscriptionName -Current
        Set-AzureSubscription -SubscriptionName $subscriptionName -CurrentStorageAccountName $storageAccount

        #create an empty cloud service
        New-AzureService -ServiceName $serviceName -Label "hkcass$region" -Location $azureRegion
        Write-Host "Created $serviceName"

        $VMList= @()   # stores the list of azure vm configuration objects
        #create the list of VMs
        foreach($vmName in $vmNames)
        {
           $VMList += New-AzureVMConfig -Name $vmName -InstanceSize ExtraSmall -ImageName $imageName |
           Add-AzureProvisioningConfig -Linux -LinuxUser "localadmin" -Password "Local123" |
           Set-AzureSubnet "data"
        }

        New-AzureVM -ServiceName $serviceName -VNetName $vnetName -VMs $VMList

        #Create internal load balancer
        Add-AzureInternalLoadBalancer -ServiceName $serviceName -InternalLoadBalancerName $ilbName -SubnetName "data" -StaticVNetIPAddress "$ilbIP"
        Write-Host "Created $ilbName"
        #Add add the thrift endpoint to the internal load balancer for all the VMs
        foreach($vmName in $vmNames)
        {
            Get-AzureVM -ServiceName $serviceName -Name $vmName |
                Add-AzureEndpoint -Name $thriftEndPoint -LBSetName "ThriftLBSet" -Protocol tcp -LocalPort 9160 -PublicPort 9160 -ProbePort 9160 -ProbeProtocol tcp -ProbeIntervalInSeconds 10 -InternalLoadBalancerName $ilbName |
                Update-AzureVM

            Write-Host "created $vmName"     
        }

**Krok 3: Konfigurowanie Cassandra na każdej maszynie Wirtualnej**

Zaloguj się do maszyny Wirtualnej i wykonaj następujące czynności:

* Edytuj $CASS_HOME/conf/cassandra-rackdc.properties do określania właściwości stojaku i centrum danych:
  
       dc =EASTUS, rack =rack1
* Edytuj cassandra.yaml do konfiguracji węzłów inicjatora, jak pokazano poniżej:
  
       Seeds: "10.1.2.4,10.1.2.6,10.1.2.8,10.1.2.10"

**Krok 4: Uruchom maszyn wirtualnych i przetestowanie klastra**

Zaloguj się do jednego z węzłów (np. hk-c1-zachód us) i uruchom następujące polecenie, aby wyświetlić stan klastra:

       nodetool –h 10.1.2.4 –p 7199 status

Powinien zostać wyświetlony ekran podobny do przedstawionego poniżej 8 węzłów klastra:

<table>
<tr><th>Stan</th><th>Adres    </th><th>Ładowanie    </th><th>Tokeny    </th><th>Właścicielem </th><th>Identyfikator hosta    </th><th>Stojak</th></tr>
<tr><th>WYREJESTRUJ    </td><td>10.1.2.4     </td><td>87.81 KB    </td><td>256    </td><td>38.0%    </td><td>Identyfikator GUID (usunięte)</td><td>szafa1</td></tr>
<tr><th>WYREJESTRUJ    </td><td>10.1.2.5     </td><td>41.08 KB    </td><td>256    </td><td>68.9%    </td><td>Identyfikator GUID (usunięte)</td><td>szafa1</td></tr>
<tr><th>WYREJESTRUJ    </td><td>10.1.2.6     </td><td>55.29 KB    </td><td>256    </td><td>68.8%    </td><td>Identyfikator GUID (usunięte)</td><td>szafa2</td></tr>
<tr><th>WYREJESTRUJ    </td><td>10.1.2.7     </td><td>55.29 KB    </td><td>256    </td><td>68.8%    </td><td>Identyfikator GUID (usunięte)</td><td>szafa2</td></tr>
<tr><th>WYREJESTRUJ    </td><td>10.1.2.8     </td><td>55.29 KB    </td><td>256    </td><td>68.8%    </td><td>Identyfikator GUID (usunięte)</td><td>szafa3</td></tr>
<tr><th>WYREJESTRUJ    </td><td>10.1.2.9     </td><td>55.29 KB    </td><td>256    </td><td>68.8%    </td><td>Identyfikator GUID (usunięte)</td><td>szafa3</td></tr>
<tr><th>WYREJESTRUJ    </td><td>10.1.2.10     </td><td>55.29 KB    </td><td>256    </td><td>68.8%    </td><td>Identyfikator GUID (usunięte)</td><td>rack4</td></tr>
<tr><th>WYREJESTRUJ    </td><td>10.1.2.11     </td><td>55.29 KB    </td><td>256    </td><td>68.8%    </td><td>Identyfikator GUID (usunięte)</td><td>rack4</td></tr>
</table>

## <a name="test-the-single-region-cluster"></a>Przetestowanie klastra pojedynczy Region
Aby przetestować klastra, wykonaj następujące kroki:

1. Za pomocą polecenia Get-AzureInternalLoadbalancer polecenia programu Powershell, Uzyskaj adres IP wewnętrznego modułu równoważenia obciążenia (np.  10.1.2.101). Poniżej przedstawiono składnię polecenia: Get-AzureLoadbalancer — ServiceName "hk — c-svc zachód us" [Wyświetla szczegóły wewnętrznego modułu równoważenia obciążenia oraz adres IP]
2. Zaloguj się do maszyny Wirtualnej (np. hk-w1 — zachód us) kolektywu serwerów sieci web przy użyciu programu Putty lub ssh
3. Wykonanie $CASS_HOME/bin/cqlsh 10.1.2.101 9160
4. Aby sprawdzić, czy klaster działa, należy użyć następujących poleceń CQL:
   
     TWORZENIE przestrzeni KLUCZY customers_ks z REPLIKACJĄ = {"class": "SimpleStrategy", "replication_factor": 3};   UŻYJ customers_ks;   Tworzenie tabeli Customers(customer_id int PRIMARY KEY, firstname text, lastname text);   Wstaw do Customers(customer_id, firstname, lastname) VALUES(1, 'John', 'Doe');   Wstaw do Customers(customer_id, firstname, lastname) wartości 2, 'Magdalena', 'Nowak';
   
     Wybierz * od klientów.

Powinien zostać wyświetlony ekran, tak jak poniżej:

<table>
  <tr><th> customer_id </th><th> Imię </th><th> nazwisko </th></tr>
  <tr><td> 1 </td><td> Jan </td><td> Nowak </td></tr>
  <tr><td> 2 </td><td> Magdalena </td><td> Nowak </td></tr>
</table>

Należy pamiętać, że przestrzeni kluczy, utworzony w kroku 4 używa SimpleStrategy z replication_factor 3. SimpleStrategy jest zalecane dla pojedynczego wdrożenia Centrum natomiast NetworkTopologyStrategy danych z wielu centrów. Replication_factor 3 zapewni tolerancji na wypadek awarii węzła.

## <a id="tworegion"></a>Proces w przypadku wdrażania
Zostanie korzystaj z wdrożenia pojedynczego obszaru ukończone i powtórz ten sam proces instalacji drugiego region. Klucza różnica między wdrażania jednego lub wielu region jest instalację tunelu sieci VPN na potrzeby komunikacji między regionu; Firma Microsoft będzie rozpoczynać się od instalacji sieciowej, obsługi administracyjnej maszyn wirtualnych i skonfiguruj Cassandra.

### <a name="step-1-create-the-virtual-network-at-the-2nd-region"></a>Krok 1: Utwórz sieć wirtualną w regionie 2
Zaloguj się do klasycznego portalu Azure i utworzyć sieć wirtualną z programem atrybutów w tabeli. Zobacz [skonfigurować sieć wirtualną Cloud-Only w klasycznym portalu Azure](../../../virtual-network/virtual-networks-create-vnet-classic-pportal.md) szczegółowy opis kroków procesu.      

<table>
<tr><th>Nazwa atrybutu    </th><th>Wartość    </th><th>Uwagi</th></tr>
<tr><td>Nazwa    </td><td>sieci wirtualnej przypadku wschód us</td><td></td></tr>
<tr><td>Region    </td><td>Wschodnie stany USA</td><td></td></tr>
<tr><td>Serwery DNS        </td><td></td><td>Zignoruj ten komunikat, ponieważ nie używamy serwera DNS</td></tr>
<tr><td>Konfigurowanie sieci VPN punkt lokacja</td><td></td><td>        Zignoruj ten komunikat</td></tr>
<tr><td>Konfiguracja tunelu sieci VPN łączącego lokacje</td><td></td><td>        Zignoruj ten komunikat</td></tr>
<tr><td>Przestrzeń adresów    </td><td>10.2.0.0/16</td><td></td></tr>
<tr><td>Uruchamianie adresu IP    </td><td>10.2.0.0    </td><td></td></tr>
<tr><td>CIDR    </td><td>/16 (65531)</td><td></td></tr>
</table>

Dodaj następujące podsieci:

<table>
<tr><th>Nazwa    </th><th>Uruchamianie adresu IP    </th><th>CIDR    </th><th>Uwagi</th></tr>
<tr><td>sieci Web    </td><td>10.2.1.0    </td><td>/24 (251)    </td><td>Podsieć dla kolektywu serwerów sieci web</td></tr>
<tr><td>Dane    </td><td>10.2.2.0    </td><td>/24 (251)    </td><td>Podsieć dla węzłów bazy danych</td></tr>
</table>


### <a name="step-2-create-local-networks"></a>Krok 2: Tworzenie sieci lokalne
Sieci lokalnej w sieci wirtualnej platformy Azure jest przestrzeń adresów serwera proxy, która mapuje z lokacją zdalną, w tym chmurę prywatną lub inny region platformy Azure. Ta przestrzeń adresowa serwera proxy jest powiązany z bramy zdalnego routingu sieci do prawej sieci miejsc docelowych. Zobacz [Konfigurowanie sieci wirtualnej do sieci wirtualnej połączenia](../../../vpn-gateway/virtual-networks-configure-vnet-to-vnet-connection.md) instrukcje dotyczące nawiązywania połączenia do Wirtualnymi.

Utwórz dwie sieci lokalne na następujące informacje:

| Nazwa sieci | Adres bramy sieci VPN | Przestrzeń adresów | Uwagi |
| --- | --- | --- | --- |
| HK-lnet-map-to-East-US |23.1.1.1 |10.2.0.0/16 |Podczas tworzenia sieci lokalnej do symbolu zastępczego adres bramy. Adres bramy rzeczywistych jest wypełniony, po utworzeniu bramy. Upewnij się, że przestrzeń adresowa dokładnie odpowiada odpowiednich zdalna sieć wirtualna; w takim przypadku w regionie wschodnie stany USA utworzona sieć wirtualna. |
| HK-lnet-map-to-West-US |23.2.2.2 |10.1.0.0/16 |Podczas tworzenia sieci lokalnej do symbolu zastępczego adres bramy. Adres bramy rzeczywistych jest wypełniony, po utworzeniu bramy. Upewnij się, że przestrzeń adresowa dokładnie odpowiada odpowiednich zdalna sieć wirtualna; w takim przypadku utworzona sieć wirtualna regionu zachodnie stany USA. |

### <a name="step-3-map-local-network-to-the-respective-vnets"></a>Krok 3: Sieć "Lokalnie" mapy do odpowiednich sieci wirtualnych
W klasycznym portalu Azure wybierz każdej sieci wirtualnej, kliknij przycisk "Konfiguruj" Sprawdź "Połącz z sieci lokalnej" i wybierz sieci lokalnej na następujące informacje:

| Virtual Network | Sieci lokalnej |
| --- | --- |
| HK — sieci wirtualnej zachód us |HK-lnet-map-to-East-US |
| HK — sieci wirtualnej wschód us |HK-lnet-map-to-West-US |

### <a name="step-4-create-gateways-on-vnet1-and-vnet2"></a>Krok 4: Tworzenie bramy na VNET1 i VNET2
Z poziomu pulpitu nawigacyjnego sieci wirtualnych kliknij przycisk Utwórz bramy, które wyzwoli bramy sieci VPN, w procesie inicjowania obsługi. Po kilku minutach adres bramy rzeczywiste powinien być wyświetlany pulpit nawigacyjny w każdej sieci wirtualnej.

### <a name="step-5-update-local-networks-with-the-respective-gateway-addresses"></a>Krok 5: Aktualizacji "Lokalnie" sieci z adresami odpowiednich "brama"
Edytuj zarówno lokalnej sieci należy zastąpić adres IP bramy symbol zastępczy adres IP rzeczywista właśnie elastycznie bramy. Użyj następującego mapowania:

<table>
<tr><th>Sieci lokalnej    </th><th>Brama sieci wirtualnej</th></tr>
<tr><td>HK-lnet-map-to-East-US </td><td>Brama hk — sieci wirtualnej zachód us</td></tr>
<tr><td>HK-lnet-map-to-West-US </td><td>Brama hk — sieci wirtualnej wschód us</td></tr>
</table>

### <a name="step-6-update-the-shared-key"></a>Krok 6: Zaktualizuj klucza wspólnego
Użyj następującego skryptu programu Powershell można zaktualizować klucza IPSec każdej bramy sieci VPN [Użyj klucza sake dla obu bram]: Set AzureVNetGatewayKey - VNetName hk — sieci wirtualnej wschód us - LocalNetworkSiteName hk-lnet-map-to-west-us - SharedKey D9E76BKK Zestaw AzureVNetGatewayKey - VNetName hk — sieci wirtualnej zachód us - LocalNetworkSiteName hk-lnet-map-to-east-us - SharedKey D9E76BKK

### <a name="step-7-establish-the-vnet-to-vnet-connection"></a>Krok 7: Nawiązania połączenia do Wirtualnymi
Z klasycznego portalu Azure użyj menu "Pulpitu NAWIGACYJNEGO" sieci wirtualnej do ustanawiania połączenia brama brama. Używanie elementów menu "POŁĄCZ" w dolnym pasku narzędzi. Po kilku minutach pulpitu nawigacyjnego powinien być wyświetlany szczegóły połączenia graficznie.

### <a name="step-8-create-the-virtual-machines-in-region-2"></a>Krok 8: Tworzenie maszyn wirtualnych w regionie #2
Utwórz obraz Ubuntu zgodnie z opisem w regionie #1 wdrożenia, wykonując kroki tego samego lub kopiowania pliku obrazu dysku VHD do konta magazynu Azure znajduje się w regionie #2 a obrazu. Skorzystaj z tego obrazu i Utwórz następującą listę maszyn wirtualnych do nowej usługi w chmurze hk-c-svc wschód nam:

| Nazwa komputera | Podsieć | Adres IP | Zestaw dostępności | DC/Rack | Inicjatora? |
| --- | --- | --- | --- | --- | --- |
| HK-c1-wschód us |Dane |10.2.2.4 |HK-c-aset-1 |DC = stojak EASTUS = szafa1 |Tak |
| HK-c2-wschód us |Dane |10.2.2.5 |HK-c-aset-1 |DC = stojak EASTUS = szafa1 |Nie |
| HK-c3-wschód us |Dane |10.2.2.6 |HK-c-aset-1 |DC = stojak EASTUS = szafa2 |Tak |
| HK-c5-wschód us |Dane |10.2.2.8 |HK-c-aset-2 |DC = stojak EASTUS = szafa3 |Tak |
| HK-c6-wschód us |Dane |10.2.2.9 |HK-c-aset-2 |DC = stojak EASTUS = szafa3 |Nie |
| HK-c7-wschód us |Dane |10.2.2.10 |HK-c-aset-2 |DC = stojak EASTUS = rack4 |Tak |
| HK-c8-wschód us |Dane |10.2.2.11 |HK-c-aset-2 |DC = stojak EASTUS = rack4 |Nie |
| HK-w1 — wschód us |sieci Web |10.2.1.4 |HK p-aset-1 |Nie dotyczy |Nie dotyczy |
| HK-w2 — wschód us |sieci Web |10.2.1.5 |HK p-aset-1 |Nie dotyczy |Nie dotyczy |

Postępuj zgodnie z instrukcjami region #1, ale użyj 10.2.xxx.xxx przestrzeni adresowej.

### <a name="step-9-configure-cassandra-on-each-vm"></a>Krok 9: Konfigurowanie Cassandra na każdej maszynie Wirtualnej
Zaloguj się do maszyny Wirtualnej i wykonaj następujące czynności:

1. Edytuj $CASS_HOME/conf/cassandra-rackdc.properties do określania właściwości stojaku i centrum danych w formacie: dc = stojak EASTUS = szafa1
2. Edytuj cassandra.yaml do konfigurowania węzłów inicjatora: ziarna: "10.1.2.4,10.1.2.6,10.1.2.8,10.1.2.10,10.2.2.4,10.2.2.6,10.2.2.8,10.2.2.10"

### <a name="step-10-start-cassandra"></a>Krok 10: Uruchom Cassandra
Zaloguj się do każdej maszyny Wirtualnej i uruchom Cassandra w tle, uruchamiając następujące polecenie: $CASS_HOME/bin/cassandra

## <a name="test-the-multi-region-cluster"></a>Przetestowanie klastra w przypadku
Już Cassandra została wdrożona do 16 węzłów o 8 węzłów w każdym regionie Azure. Te węzły znajdują się w tym samym klastrze z wspólnej nazwy klastra i konfiguracja węzła inicjatora. Aby przetestować klastra, należy wykonać poniższe czynności:

### <a name="step-1-get-the-internal-load-balancer-ip-for-both-the-regions-using-powershell"></a>Krok 1: Uzyskiwanie adresu IP usługi równoważenia obciążenia wewnętrznego dla regionów przy użyciu programu PowerShell
* Get-AzureInternalLoadbalancer - ServiceName "hk — c-svc zachód us"
* Get-AzureInternalLoadbalancer - ServiceName "hk — c-svc wschód us"  
  
    Należy zwrócić uwagę na adresy IP (np. - 10.1.2.101, wschód - zachód 10.2.2.101) wyświetlane.

### <a name="step-2-execute-the-following-in-the-west-region-after-logging-into-hk-w1-west-us"></a>Krok 2: Wykonaj następujące czynności w regionie Zachód po zalogowaniu w hk-w1 — zachód us
1. Wykonanie $CASS_HOME/bin/cqlsh 10.1.2.101 9160
2. Uruchom następujące polecenia CQL:
   
     TWORZENIE przestrzeni KLUCZY customers_ks z REPLIKACJĄ = {"class": "NetworkToplogyStrategy", "WESTUS": 3, EASTUS: 3};   UŻYJ customers_ks;   Tworzenie tabeli Customers(customer_id int PRIMARY KEY, firstname text, lastname text);   Wstaw do Customers(customer_id, firstname, lastname) VALUES(1, 'John', 'Doe');   Wstaw do Customers(customer_id, firstname, lastname) wartości 2, 'Magdalena', 'Nowak';   Wybierz * od klientów.

Powinien zostać wyświetlony ekran, tak jak poniżej:

| customer_id | Imię | nazwisko |
| --- | --- | --- |
| 1 |Jan |Nowak |
| 2 |Magdalena |Nowak |

### <a name="step-3-execute-the-following-in-the-east-region-after-logging-into-hk-w1-east-us"></a>Krok 3: Wykonaj następujące czynności w regionie wschodnie po zalogowaniu w hk-w1 — wschód us:
1. Wykonanie $CASS_HOME/bin/cqlsh 10.2.2.101 9160
2. Uruchom następujące polecenia CQL:
   
     UŻYJ customers_ks;   Tworzenie tabeli Customers(customer_id int PRIMARY KEY, firstname text, lastname text);   Wstaw do Customers(customer_id, firstname, lastname) VALUES(1, 'John', 'Doe');   Wstaw do Customers(customer_id, firstname, lastname) wartości 2, 'Magdalena', 'Nowak';   Wybierz * od klientów.

Wyświetlanie tego samego powinny być widoczne, widziany dla regionu zachodnie:

| customer_id | Imię | nazwisko |
| --- | --- | --- |
| 1 |Jan |Nowak |
| 2 |Magdalena |Nowak |

Wykonaj kilka więcej operacji wstawienia i zobacz, czy te replikowane Zachodnie-us, które są częścią klastra.

## <a name="test-cassandra-cluster-from-nodejs"></a>Testowanie Cassandra klastra w oprogramowaniu Node.js
Przy użyciu jednej z maszyn wirtualnych systemu Linux utworzone w "web" warstwy wcześniej, firma Microsoft będzie wykonywał prostego skryptu Node.js odczytać wcześniej wstawionych danych

**Krok 1: Instalacja klienta Cassandra i Node.js**

1. Instalowanie środowiska Node.js i npm
2. Instalowanie pakietu "cassandra klienta w węźle" za pomocą programu npm
3. Uruchom następujący skrypt w wierszu polecenia powłoki, który zawiera ciąg json pobrane dane:
   
        var pooledCon = require('cassandra-client').PooledConnection;
        var ksName = "custsupport_ks";
        var cfName = "customers_cf";
        var hostList = ['internal_loadbalancer_ip:9160'];
        var ksConOptions = { hosts: hostList,
                             keyspace: ksName, use_bigints: false };
   
        function createKeyspace(callback){
           var cql = 'CREATE KEYSPACE ' + ksName + ' WITH strategy_class=SimpleStrategy AND strategy_options:replication_factor=1';
           var sysConOptions = { hosts: hostList,  
                                 keyspace: 'system', use_bigints: false };
           var con = new pooledCon(sysConOptions);
           con.execute(cql,[],function(err) {
           if (err) {
             console.log("Failed to create Keyspace: " + ksName);
             console.log(err);
           }
           else {
             console.log("Created Keyspace: " + ksName);
             callback(ksConOptions, populateCustomerData);
           }
           });
           con.shutdown();
        }
   
        function createColumnFamily(ksConOptions, callback){
          var params = ['customers_cf','custid','varint','custname',
                        'text','custaddress','text'];
          var cql = 'CREATE COLUMNFAMILY ? (? ? PRIMARY KEY,? ?, ? ?)';
        var con =  new pooledCon(ksConOptions);
          con.execute(cql,params,function(err) {
              if (err) {
                 console.log("Failed to create column family: " + params[0]);
                 console.log(err);
              }
              else {
                 console.log("Created column family: " + params[0]);
                 callback();
              }
          });
          con.shutdown();
        }
   
        //populate Data
        function populateCustomerData() {
           var params = ['John','Infinity Dr, TX', 1];
           updateCustomer(ksConOptions,params);
   
           params = ['Tom','Fermat Ln, WA', 2];
           updateCustomer(ksConOptions,params);
        }
   
        //update will also insert the record if none exists
        function updateCustomer(ksConOptions,params)
        {
          var cql = 'UPDATE customers_cf SET custname=?,custaddress=? where custid=?';
          var con = new pooledCon(ksConOptions);
          con.execute(cql,params,function(err) {
              if (err) console.log(err);
              else console.log("Inserted customer : " + params[0]);
          });
          con.shutdown();
        }
   
        //read the two rows inserted above
        function readCustomer(ksConOptions)
        {
          var cql = 'SELECT * FROM customers_cf WHERE custid IN (1,2)';
          var con = new pooledCon(ksConOptions);
          con.execute(cql,[],function(err,rows) {
              if (err)
                 console.log(err);
              else
                 for (var i=0; i<rows.length; i++)
                    console.log(JSON.stringify(rows[i]));
            });
           con.shutdown();
        }
   
        //exectue the code
        createKeyspace(createColumnFamily);
        readCustomer(ksConOptions)

## <a name="conclusion"></a>Podsumowanie
Microsoft Azure to elastyczna platforma, która umożliwia uruchamianie zarówno firmy Microsoft, jak i oprogramowanie typu open source, jak zostało to pokazane w tym ćwiczeniu. Klastry Cassandra wysokiej dostępności można wdrożyć na jednego centrum danych przez rozłożenie węzły klastra w wielu domenach awarii. Klastry Cassandra można także wdrożyć w różnych regionach platformy Azure odległymi geograficznie systemów potwierdzającego po awarii. Azure i Cassandra razem umożliwia konstruowanie wysoce skalowalną, wysokiej dostępności i usługi w chmurze możliwych do odzyskania po awarii wymagane przez internet współczesnych skalowania usług.  

## <a name="references"></a>Dokumentacja
* [http://cassandra.apache.org](http://cassandra.apache.org)
* [http://www.datastax.com](http://www.datastax.com)
* [http://www.nodejs.org](http://www.nodejs.org)

